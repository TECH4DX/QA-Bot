[2022-09-14 16:43:50,587][__main__][INFO] - start sentence preprocess...
[2022-09-14 16:43:50,588][__main__][INFO] - 
sentence: 歌曲《人生长路》出自刘德华国语专辑《男人的爱》，由李泉作词作曲，2001年出行发版
chinese_split: True
replace_entity_with_type:  True
replace_entity_with_scope: True
tokens:    ['歌曲', '《', 'TAIL_歌曲', '》', '出自', '刘德华', '国语专辑', '《', 'HEAD_所属专辑', '》', '，', '由', '李泉', '作词', '作曲', '，', '2001', '年', '出行', '发版']
token2idx: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 32, 1, 1, 1, 1, 32, 1, 1, 1, 1]
length:    20
head_idx:  8
tail_idx:  2
[2022-09-14 16:43:50,588][__main__][INFO] - device: cpu
[2022-09-14 16:43:50,595][__main__][INFO] - model name: transformer
[2022-09-14 16:43:50,595][__main__][INFO] - 
 Transformer(
  (embedding): Embedding(
    (wordEmbed): Embedding(445, 60, padding_idx=0)
    (entityPosEmbed): Embedding(62, 60, padding_idx=0)
    (attribute_keyPosEmbed): Embedding(62, 60, padding_idx=0)
    (layer_norm): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
  )
  (transformer): Transformer(
    (layer): ModuleList(
      (0): TransformerLayer(
        (attention): TransformerAttention(
          (multihead_attention): MultiHeadAttention(
            (q_in): Linear(in_features=60, out_features=60, bias=True)
            (k_in): Linear(in_features=60, out_features=60, bias=True)
            (v_in): Linear(in_features=60, out_features=60, bias=True)
            (attention): DotAttention()
            (out): Linear(in_features=60, out_features=60, bias=True)
          )
          (dense): Linear(in_features=60, out_features=60, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (layerNorm): LayerNorm((60,), eps=1e-12, elementwise_affine=True)
        )
        (output): TransformerOutput(
          (zoom_in): Linear(in_features=60, out_features=256, bias=True)
          (zoom_out): Linear(in_features=256, out_features=60, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (layerNorm): LayerNorm((60,), eps=1e-12, elementwise_affine=True)
        )
      )
      (1): TransformerLayer(
        (attention): TransformerAttention(
          (multihead_attention): MultiHeadAttention(
            (q_in): Linear(in_features=60, out_features=60, bias=True)
            (k_in): Linear(in_features=60, out_features=60, bias=True)
            (v_in): Linear(in_features=60, out_features=60, bias=True)
            (attention): DotAttention()
            (out): Linear(in_features=60, out_features=60, bias=True)
          )
          (dense): Linear(in_features=60, out_features=60, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (layerNorm): LayerNorm((60,), eps=1e-12, elementwise_affine=True)
        )
        (output): TransformerOutput(
          (zoom_in): Linear(in_features=60, out_features=256, bias=True)
          (zoom_out): Linear(in_features=256, out_features=60, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (layerNorm): LayerNorm((60,), eps=1e-12, elementwise_affine=True)
        )
      )
      (2): TransformerLayer(
        (attention): TransformerAttention(
          (multihead_attention): MultiHeadAttention(
            (q_in): Linear(in_features=60, out_features=60, bias=True)
            (k_in): Linear(in_features=60, out_features=60, bias=True)
            (v_in): Linear(in_features=60, out_features=60, bias=True)
            (attention): DotAttention()
            (out): Linear(in_features=60, out_features=60, bias=True)
          )
          (dense): Linear(in_features=60, out_features=60, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (layerNorm): LayerNorm((60,), eps=1e-12, elementwise_affine=True)
        )
        (output): TransformerOutput(
          (zoom_in): Linear(in_features=60, out_features=256, bias=True)
          (zoom_out): Linear(in_features=256, out_features=60, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (layerNorm): LayerNorm((60,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (fc): Linear(in_features=60, out_features=15, bias=True)
)
