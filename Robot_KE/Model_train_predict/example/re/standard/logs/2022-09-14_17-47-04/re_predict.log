[2022-09-14 17:47:05,066][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:47:06,212][__main__][INFO] - device: cpu
[2022-09-14 17:47:09,482][__main__][INFO] - model name: lm
[2022-09-14 17:47:09,483][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:47:09,550][__main__][INFO] - "pip方式" 和 "MindSpore Ascend" 在句中关系为："安装"，置信度为0.99。
[2022-09-14 17:47:09,550][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:47:10,578][__main__][INFO] - device: cpu
[2022-09-14 17:47:12,298][__main__][INFO] - model name: lm
[2022-09-14 17:47:12,298][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:47:12,363][__main__][INFO] - "MindSpore Ascend" 和 "pip方式" 在句中关系为："安装"，置信度为0.99。
[2022-09-14 17:47:12,364][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:47:13,349][__main__][INFO] - device: cpu
[2022-09-14 17:47:15,089][__main__][INFO] - model name: lm
[2022-09-14 17:47:15,090][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:47:15,147][__main__][INFO] - "pip方式" 和 "910" 在句中关系为："版本"，置信度为0.99。
[2022-09-14 17:47:15,148][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:47:16,134][__main__][INFO] - device: cpu
[2022-09-14 17:47:17,875][__main__][INFO] - model name: lm
[2022-09-14 17:47:17,876][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:47:17,931][__main__][INFO] - "910" 和 "pip方式" 在句中关系为："版本"，置信度为0.99。
[2022-09-14 17:47:17,932][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:47:18,913][__main__][INFO] - device: cpu
[2022-09-14 17:47:20,642][__main__][INFO] - model name: lm
[2022-09-14 17:47:20,643][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:47:20,701][__main__][INFO] - "MindSpore Ascend" 和 "910" 在句中关系为："版本"，置信度为1.00。
[2022-09-14 17:47:20,701][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:47:21,681][__main__][INFO] - device: cpu
[2022-09-14 17:47:23,329][__main__][INFO] - model name: lm
[2022-09-14 17:47:23,329][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:47:23,384][__main__][INFO] - "910" 和 "MindSpore Ascend" 在句中关系为："版本"，置信度为1.00。
[2022-09-14 17:47:23,385][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:47:24,356][__main__][INFO] - device: cpu
[2022-09-14 17:47:25,998][__main__][INFO] - model name: lm
[2022-09-14 17:47:25,999][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:47:26,039][__main__][INFO] - "pip方式" 和 "MindSpore" 在句中关系为："安装"，置信度为0.99。
[2022-09-14 17:47:26,040][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:47:27,029][__main__][INFO] - device: cpu
[2022-09-14 17:47:28,678][__main__][INFO] - model name: lm
[2022-09-14 17:47:28,678][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:47:28,719][__main__][INFO] - "MindSpore" 和 "pip方式" 在句中关系为："安装"，置信度为0.99。
[2022-09-14 17:47:28,720][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:47:29,728][__main__][INFO] - device: cpu
[2022-09-14 17:47:31,381][__main__][INFO] - model name: lm
[2022-09-14 17:47:31,382][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:47:31,432][__main__][INFO] - "pip方式" 和 "Ascend" 在句中关系为："安装"，置信度为0.99。
[2022-09-14 17:47:31,433][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:47:32,416][__main__][INFO] - device: cpu
[2022-09-14 17:47:34,060][__main__][INFO] - model name: lm
[2022-09-14 17:47:34,061][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:47:34,109][__main__][INFO] - "Ascend" 和 "pip方式" 在句中关系为："安装"，置信度为0.99。
[2022-09-14 17:47:34,110][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:47:35,082][__main__][INFO] - device: cpu
[2022-09-14 17:47:36,720][__main__][INFO] - model name: lm
[2022-09-14 17:47:36,720][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:47:36,766][__main__][INFO] - "pip方式" 和 "91" 在句中关系为："版本"，置信度为0.99。
[2022-09-14 17:47:36,767][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:47:37,747][__main__][INFO] - device: cpu
[2022-09-14 17:47:39,402][__main__][INFO] - model name: lm
[2022-09-14 17:47:39,403][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:47:39,449][__main__][INFO] - "91" 和 "pip方式" 在句中关系为："版本"，置信度为0.99。
[2022-09-14 17:47:39,450][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:47:40,422][__main__][INFO] - device: cpu
[2022-09-14 17:47:42,063][__main__][INFO] - model name: lm
[2022-09-14 17:47:42,064][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:47:42,114][__main__][INFO] - "pip方式" 和 "mindspore-ascend" 在句中关系为："安装"，置信度为0.99。
[2022-09-14 17:47:42,115][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:47:43,083][__main__][INFO] - device: cpu
[2022-09-14 17:47:44,734][__main__][INFO] - model name: lm
[2022-09-14 17:47:44,734][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:47:44,783][__main__][INFO] - "mindspore-ascend" 和 "pip方式" 在句中关系为："安装"，置信度为0.99。
[2022-09-14 17:47:44,784][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:47:45,752][__main__][INFO] - device: cpu
[2022-09-14 17:47:47,342][__main__][INFO] - model name: lm
[2022-09-14 17:47:47,342][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:47:47,383][__main__][INFO] - "pip方式" 和 "910" 在句中关系为："版本"，置信度为0.99。
[2022-09-14 17:47:47,384][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:47:48,354][__main__][INFO] - device: cpu
[2022-09-14 17:47:49,997][__main__][INFO] - model name: lm
[2022-09-14 17:47:49,998][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:47:50,038][__main__][INFO] - "910" 和 "pip方式" 在句中关系为："版本"，置信度为0.99。
[2022-09-14 17:47:50,038][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:47:51,004][__main__][INFO] - device: cpu
[2022-09-14 17:47:52,665][__main__][INFO] - model name: lm
[2022-09-14 17:47:52,666][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:47:52,717][__main__][INFO] - "pip方式" 和 "版本" 在句中关系为："安装"，置信度为0.95。
[2022-09-14 17:47:52,717][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:47:53,687][__main__][INFO] - device: cpu
[2022-09-14 17:47:55,300][__main__][INFO] - model name: lm
[2022-09-14 17:47:55,301][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:47:55,340][__main__][INFO] - "版本" 和 "pip方式" 在句中关系为："安装"，置信度为0.96。
[2022-09-14 17:47:55,341][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:47:56,307][__main__][INFO] - device: cpu
[2022-09-14 17:47:57,967][__main__][INFO] - model name: lm
[2022-09-14 17:47:57,967][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:47:58,016][__main__][INFO] - "MindSpore" 和 "Ascend" 在句中关系为："安装"，置信度为0.99。
[2022-09-14 17:47:58,017][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:47:58,981][__main__][INFO] - device: cpu
[2022-09-14 17:48:00,600][__main__][INFO] - model name: lm
[2022-09-14 17:48:00,600][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:48:00,641][__main__][INFO] - "Ascend" 和 "MindSpore" 在句中关系为："安装"，置信度为0.99。
[2022-09-14 17:48:00,642][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:48:01,614][__main__][INFO] - device: cpu
[2022-09-14 17:48:03,279][__main__][INFO] - model name: lm
[2022-09-14 17:48:03,280][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:48:03,329][__main__][INFO] - "MindSpore" 和 "91" 在句中关系为："版本"，置信度为1.00。
[2022-09-14 17:48:03,330][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:48:04,295][__main__][INFO] - device: cpu
[2022-09-14 17:48:05,937][__main__][INFO] - model name: lm
[2022-09-14 17:48:05,937][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:48:05,978][__main__][INFO] - "91" 和 "MindSpore" 在句中关系为："版本"，置信度为1.00。
[2022-09-14 17:48:05,979][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:48:06,952][__main__][INFO] - device: cpu
[2022-09-14 17:48:08,607][__main__][INFO] - model name: lm
[2022-09-14 17:48:08,607][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:48:08,649][__main__][INFO] - "MindSpore" 和 "mindspore-ascend" 在句中关系为："安装"，置信度为0.99。
[2022-09-14 17:48:08,650][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:48:09,617][__main__][INFO] - device: cpu
[2022-09-14 17:48:11,199][__main__][INFO] - model name: lm
[2022-09-14 17:48:11,199][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:48:11,249][__main__][INFO] - "mindspore-ascend" 和 "MindSpore" 在句中关系为："安装"，置信度为0.99。
[2022-09-14 17:48:11,250][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:48:11,798][__main__][INFO] - device: cpu
[2022-09-14 17:48:13,026][__main__][INFO] - model name: lm
[2022-09-14 17:48:13,027][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:48:13,065][__main__][INFO] - "MindSpore" 和 "910" 在句中关系为："版本"，置信度为1.00。
[2022-09-14 17:48:13,066][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:48:13,604][__main__][INFO] - device: cpu
[2022-09-14 17:48:14,772][__main__][INFO] - model name: lm
[2022-09-14 17:48:14,772][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:48:14,819][__main__][INFO] - "910" 和 "MindSpore" 在句中关系为："版本"，置信度为1.00。
[2022-09-14 17:48:14,819][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:48:15,366][__main__][INFO] - device: cpu
[2022-09-14 17:48:16,549][__main__][INFO] - model name: lm
[2022-09-14 17:48:16,549][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:48:16,590][__main__][INFO] - "MindSpore" 和 "版本" 在句中关系为："使用安装"，置信度为0.78。
[2022-09-14 17:48:16,591][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:48:17,620][__main__][INFO] - device: cpu
[2022-09-14 17:48:19,282][__main__][INFO] - model name: lm
[2022-09-14 17:48:19,283][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:48:19,322][__main__][INFO] - "版本" 和 "MindSpore" 在句中关系为："使用安装"，置信度为0.88。
[2022-09-14 17:48:19,323][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:48:20,362][__main__][INFO] - device: cpu
[2022-09-14 17:48:22,106][__main__][INFO] - model name: lm
[2022-09-14 17:48:22,106][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:48:22,156][__main__][INFO] - "Ascend" 和 "91" 在句中关系为："版本"，置信度为1.00。
[2022-09-14 17:48:22,157][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:48:23,192][__main__][INFO] - device: cpu
[2022-09-14 17:48:24,947][__main__][INFO] - model name: lm
[2022-09-14 17:48:24,947][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:48:24,998][__main__][INFO] - "91" 和 "Ascend" 在句中关系为："版本"，置信度为1.00。
[2022-09-14 17:48:24,999][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:48:26,037][__main__][INFO] - device: cpu
[2022-09-14 17:48:27,738][__main__][INFO] - model name: lm
[2022-09-14 17:48:27,739][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:48:27,777][__main__][INFO] - "Ascend" 和 "mindspore-ascend" 在句中关系为："安装"，置信度为0.99。
[2022-09-14 17:48:27,778][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:48:28,824][__main__][INFO] - device: cpu
[2022-09-14 17:48:30,542][__main__][INFO] - model name: lm
[2022-09-14 17:48:30,542][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:48:30,601][__main__][INFO] - "mindspore-ascend" 和 "Ascend" 在句中关系为："安装"，置信度为0.99。
[2022-09-14 17:48:30,601][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:48:31,637][__main__][INFO] - device: cpu
[2022-09-14 17:48:33,309][__main__][INFO] - model name: lm
[2022-09-14 17:48:33,309][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:48:33,360][__main__][INFO] - "Ascend" 和 "910" 在句中关系为："版本"，置信度为1.00。
[2022-09-14 17:48:33,360][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:48:34,419][__main__][INFO] - device: cpu
[2022-09-14 17:48:36,141][__main__][INFO] - model name: lm
[2022-09-14 17:48:36,141][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:48:36,199][__main__][INFO] - "910" 和 "Ascend" 在句中关系为："版本"，置信度为1.00。
[2022-09-14 17:48:36,200][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:48:37,240][__main__][INFO] - device: cpu
[2022-09-14 17:48:38,905][__main__][INFO] - model name: lm
[2022-09-14 17:48:38,906][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:48:38,945][__main__][INFO] - "Ascend" 和 "版本" 在句中关系为："使用安装"，置信度为0.90。
[2022-09-14 17:48:38,946][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:48:39,982][__main__][INFO] - device: cpu
[2022-09-14 17:48:41,636][__main__][INFO] - model name: lm
[2022-09-14 17:48:41,637][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:48:41,681][__main__][INFO] - "版本" 和 "Ascend" 在句中关系为："使用安装"，置信度为0.94。
[2022-09-14 17:48:41,682][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:48:42,711][__main__][INFO] - device: cpu
[2022-09-14 17:48:44,377][__main__][INFO] - model name: lm
[2022-09-14 17:48:44,377][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:48:44,428][__main__][INFO] - "91" 和 "mindspore-ascend" 在句中关系为："版本"，置信度为1.00。
[2022-09-14 17:48:44,428][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:48:45,456][__main__][INFO] - device: cpu
[2022-09-14 17:48:47,140][__main__][INFO] - model name: lm
[2022-09-14 17:48:47,141][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:48:47,198][__main__][INFO] - "mindspore-ascend" 和 "91" 在句中关系为："版本"，置信度为1.00。
[2022-09-14 17:48:47,199][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:48:48,237][__main__][INFO] - device: cpu
[2022-09-14 17:48:49,934][__main__][INFO] - model name: lm
[2022-09-14 17:48:49,934][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:48:49,976][__main__][INFO] - "91" 和 "910" 在句中关系为："版本"，置信度为1.00。
[2022-09-14 17:48:49,976][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:48:51,006][__main__][INFO] - device: cpu
[2022-09-14 17:48:52,715][__main__][INFO] - model name: lm
[2022-09-14 17:48:52,715][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:48:52,765][__main__][INFO] - "910" 和 "91" 在句中关系为："版本"，置信度为1.00。
[2022-09-14 17:48:52,766][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:48:53,790][__main__][INFO] - device: cpu
[2022-09-14 17:48:55,431][__main__][INFO] - model name: lm
[2022-09-14 17:48:55,431][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:48:55,479][__main__][INFO] - "91" 和 "版本" 在句中关系为："版本"，置信度为0.98。
[2022-09-14 17:48:55,479][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:48:56,497][__main__][INFO] - device: cpu
[2022-09-14 17:48:58,192][__main__][INFO] - model name: lm
[2022-09-14 17:48:58,192][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:48:58,242][__main__][INFO] - "版本" 和 "91" 在句中关系为："版本"，置信度为0.98。
[2022-09-14 17:48:58,243][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:48:59,264][__main__][INFO] - device: cpu
[2022-09-14 17:49:01,032][__main__][INFO] - model name: lm
[2022-09-14 17:49:01,033][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:49:01,082][__main__][INFO] - "mindspore-ascend" 和 "910" 在句中关系为："版本"，置信度为1.00。
[2022-09-14 17:49:01,082][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:49:02,110][__main__][INFO] - device: cpu
[2022-09-14 17:49:03,745][__main__][INFO] - model name: lm
[2022-09-14 17:49:03,745][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:49:03,794][__main__][INFO] - "910" 和 "mindspore-ascend" 在句中关系为："版本"，置信度为1.00。
[2022-09-14 17:49:03,795][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:49:04,818][__main__][INFO] - device: cpu
[2022-09-14 17:49:06,517][__main__][INFO] - model name: lm
[2022-09-14 17:49:06,517][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:49:06,564][__main__][INFO] - "mindspore-ascend" 和 "版本" 在句中关系为："安装"，置信度为0.55。
[2022-09-14 17:49:06,565][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:49:07,611][__main__][INFO] - device: cpu
[2022-09-14 17:49:09,299][__main__][INFO] - model name: lm
[2022-09-14 17:49:09,299][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:49:09,348][__main__][INFO] - "版本" 和 "mindspore-ascend" 在句中关系为："安装"，置信度为0.59。
[2022-09-14 17:49:09,349][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:49:10,375][__main__][INFO] - device: cpu
[2022-09-14 17:49:12,581][__main__][INFO] - model name: lm
[2022-09-14 17:49:12,581][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:49:12,623][__main__][INFO] - "910" 和 "版本" 在句中关系为："版本"，置信度为0.99。
[2022-09-14 17:49:12,623][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:49:13,637][__main__][INFO] - device: cpu
[2022-09-14 17:49:15,312][__main__][INFO] - model name: lm
[2022-09-14 17:49:15,313][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:49:15,352][__main__][INFO] - "版本" 和 "910" 在句中关系为："版本"，置信度为0.99。
[2022-09-14 17:49:15,353][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:49:16,371][__main__][INFO] - device: cpu
[2022-09-14 17:49:18,078][__main__][INFO] - model name: lm
[2022-09-14 17:49:18,078][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:49:18,117][__main__][INFO] - "Python" 和 "python" 在句中关系为："工具安装"，置信度为0.99。
[2022-09-14 17:49:18,118][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:49:19,128][__main__][INFO] - device: cpu
[2022-09-14 17:49:20,796][__main__][INFO] - model name: lm
[2022-09-14 17:49:20,796][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:49:20,835][__main__][INFO] - "python" 和 "Python" 在句中关系为："工具安装"，置信度为0.99。
[2022-09-14 17:49:20,835][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:49:21,831][__main__][INFO] - device: cpu
[2022-09-14 17:49:23,466][__main__][INFO] - model name: lm
[2022-09-14 17:49:23,467][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:49:23,517][__main__][INFO] - "昇腾AI处理器配套软件包" 和 "昇腾ai处理器配套软件包" 在句中关系为："工具安装"，置信度为0.99。
[2022-09-14 17:49:23,517][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:49:24,522][__main__][INFO] - device: cpu
[2022-09-14 17:49:26,220][__main__][INFO] - model name: lm
[2022-09-14 17:49:26,221][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:49:26,259][__main__][INFO] - "昇腾ai处理器配套软件包" 和 "昇腾AI处理器配套软件包" 在句中关系为："工具安装"，置信度为0.99。
[2022-09-14 17:49:26,259][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:49:27,268][__main__][INFO] - device: cpu
[2022-09-14 17:49:28,960][__main__][INFO] - model name: lm
[2022-09-14 17:49:28,961][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:49:29,016][__main__][INFO] - "GCC" 和 "gcc" 在句中关系为："工具安装"，置信度为0.99。
[2022-09-14 17:49:29,017][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:49:30,007][__main__][INFO] - device: cpu
[2022-09-14 17:49:31,680][__main__][INFO] - model name: lm
[2022-09-14 17:49:31,680][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:49:31,717][__main__][INFO] - "gcc" 和 "GCC" 在句中关系为："工具安装"，置信度为0.99。
[2022-09-14 17:49:31,718][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:49:32,717][__main__][INFO] - device: cpu
[2022-09-14 17:49:34,420][__main__][INFO] - model name: lm
[2022-09-14 17:49:34,421][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:49:34,470][__main__][INFO] - "Open MPI" 和 "open-mpi-" 在句中关系为："工具安装"，置信度为0.99。
[2022-09-14 17:49:34,470][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:49:35,471][__main__][INFO] - device: cpu
[2022-09-14 17:49:37,092][__main__][INFO] - model name: lm
[2022-09-14 17:49:37,093][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:49:37,130][__main__][INFO] - "open-mpi-" 和 "Open MPI" 在句中关系为："工具安装"，置信度为0.99。
[2022-09-14 17:49:37,131][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:49:38,126][__main__][INFO] - device: cpu
[2022-09-14 17:49:39,798][__main__][INFO] - model name: lm
[2022-09-14 17:49:39,798][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:49:39,862][__main__][INFO] - "MindSpore" 和 "mindspore" 在句中关系为："工具安装"，置信度为0.99。
[2022-09-14 17:49:39,863][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:49:40,840][__main__][INFO] - device: cpu
[2022-09-14 17:49:42,465][__main__][INFO] - model name: lm
[2022-09-14 17:49:42,465][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:49:42,502][__main__][INFO] - "mindspore" 和 "MindSpore" 在句中关系为："工具安装"，置信度为0.99。
[2022-09-14 17:49:42,503][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
[2022-09-14 17:49:43,496][__main__][INFO] - device: cpu
[2022-09-14 17:49:45,178][__main__][INFO] - model name: lm
[2022-09-14 17:49:45,178][__main__][INFO] - 
 LM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (bilstm): RNN(
    (rnn): LSTM(768, 50, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (fc): Linear(in_features=100, out_features=15, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
[2022-09-14 17:49:45,241][__main__][INFO] - "MindSpore" 和 "版本" 在句中关系为："使用安装"，置信度为0.99。
[2022-09-14 17:49:45,242][deepke.relation_extraction.standard.tools.preprocess][INFO] - use bert tokenizer...
