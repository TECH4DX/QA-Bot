mindspore.train
mindspore.train.summary
使用SummaryRecord将需要的数据存储为summary文件和lineage文件，使用方法包括自定义回调函数和自定义训练循环。保存的summary文件使用MindInsight进行可视化分析。

classmindspore.train.summary.SummaryRecord(log_dir, file_prefix='events', file_suffix='_MS', network=None, max_file_size=None, raise_exception=False, export_options=None)[源代码]
SummaryRecord用于记录summary数据和lineage数据。

该方法将在一个指定的目录中创建summary文件和lineage文件，并将数据写入文件。

它通过执行 record 方法将数据写入文件。除了通过 summary算子 记录网络的数据外，SummaryRecord还支持通过 自定义回调函数和自定义训练循环 记录数据。

Note

使用SummaryRecord时，需要将代码放置到 if __name__ == “__main__” 中运行。

确保在最后关闭SummaryRecord，否则进程不会退出。请参阅下面的示例部分，了解如何用两种方式正确关闭SummaryRecord。

每次训练只允许创建一个SummaryRecord实例，否则会导致数据写入异常。

SummaryRecord仅支持Linux系统。

编译MindSpore时，设置 -s on 关闭维测功能后，SummaryRecord不可用。

参数：

log_dir (str) - log_dir 是用来保存summary文件的目录。

file_prefix (str) - 文件的前缀。默认值：events 。

file_suffix (str) - 文件的后缀。默认值：_MS 。

network (Cell) - 表示用于保存计算图的网络。默认值：None。

max_file_size (int, 可选) - 可写入磁盘的每个文件的最大大小（以字节为单位）。例如，预期写入文件最大不超过4GB，则设置 max_file_size=4*1024**3 。默认值：None，表示无限制。

raise_exception (bool, 可选) - 设置在记录数据中发生RuntimeError或OSError异常时是否抛出异常。默认值：False，表示打印错误日志，不抛出异常。

export_options (Union[None, dict]) - 可以将保存在summary中的数据导出，并使用字典自定义所需的数据和文件格式。注：导出的文件大小不受 max_file_size 的限制。例如，您可以设置{‘tensor_format’:’npy’}将Tensor导出为 npy 文件。支持导出的数据类型如下所示。默认值：None，表示不导出数据。

tensor_format (Union[str, None]) - 自定义导出的Tensor的格式。支持[“npy”, None]。默认值：None，表示不导出Tensor。

npy：将Tensor导出为NPY文件。

异常：

TypeError： max_file_size 不是整型，或 file_prefix 和 file_suffix 不是字符串。

ValueError： 编译MindSpore时，设置 -s on 关闭了维测功能。

样例：

from mindspore.train.summary import SummaryRecord
if __name__ == '__main__':
    # use in with statement to auto close
    with SummaryRecord(log_dir="./summary_dir") as summary_record:
        pass

    # use in try .. finally .. to ensure closing
    try:
        summary_record = SummaryRecord(log_dir="./summary_dir")
    finally:
        summary_record.close()
add_value(plugin, name, value)[源代码]
添加需要记录的值。

参数：

plugin (str) - 数据类型标签。

graph：代表添加的数据为计算图。

scalar：代表添加的数据为标量。

image：代表添加的数据为图片。

tensor：代表添加的数据为张量。

histogram：代表添加的数据为直方图。

train_lineage：代表添加的数据为训练阶段的lineage数据。

eval_lineage：代表添加的数据为评估阶段的lineage数据。

dataset_graph：代表添加的数据为数据图。

custom_lineage_data：代表添加的数据为自定义lineage数据。

LANDSCAPE: 代表添加的数据为地形图。

name (str) - 数据名称。

value (Union[Tensor, GraphProto, TrainLineage, EvaluationLineage, DatasetGraph, UserDefinedInfo，LossLandscape])：待存储的值。

当plugin为”graph”时，参数值的数据类型应为”GraphProto”对象。具体详情，请参见 mindspore/ccsrc/anf_ir.proto。

当plugin为”scalar”、”image”、”tensor”或”histogram”时，参数值的数据类型应为”Tensor”对象。

当plugin为”train_lineage”时，参数值的数据类型应为”TrainLineage”对象。具体详情，请参见 mindspore/ccsrc/lineage.proto。

当plugin为”eval_lineage”时，参数值的数据类型应为”EvaluationLineage”对象。具体详情，请参见 mindspore/ccsrc/lineage.proto。

当plugin为”dataset_graph”时，参数值的数据类型应为”DatasetGraph”对象。具体详情，请参见 mindspore/ccsrc/lineage.proto。

当plugin为”custom_lineage_data”时，参数值的数据类型应为”UserDefinedInfo”对象。具体详情，请参见 mindspore/ccsrc/lineage.proto。

当plugin为”LANDSCAPE”时，参数值的数据类型应为”LossLandscape”对象。具体详情，请参见 mindspore/ccsrc/summary.proto。

异常：

ValueError： plugin 的值不在可选值内。

TypeError： name 不是非空字符串，或当 plugin 为”scalar”、”image”、”tensor”或”histogram”时，value 的数据类型不是”Tensor”对象。

样例：

from mindspore import Tensor
from mindspore.train.summary import SummaryRecord
if __name__ == '__main__':
    with SummaryRecord(log_dir="./summary_dir", file_prefix="xx_", file_suffix="_yy") as summary_record:
        summary_record.add_value('scalar', 'loss', Tensor(0.1))
close()[源代码]
将缓冲区中的数据立刻写入文件并关闭SummaryRecord。请使用with语句或try…finally语句进行自动关闭。

样例：

from mindspore.train.summary import SummaryRecord
if __name__ == '__main__':
    try:
        summary_record = SummaryRecord(log_dir="./summary_dir")
    finally:
        summary_record.close()
flush()[源代码]
刷新缓冲区，将缓冲区中的数据写入磁盘。

调用该函数以确保所有挂起事件都已写入到磁盘。

样例：

from mindspore.train.summary import SummaryRecord
if __name__ == '__main__':
    with SummaryRecord(log_dir="./summary_dir", file_prefix="xx_", file_suffix="_yy") as summary_record:
        summary_record.flush()
propertylog_dir
获取日志文件的完整路径。

返回：

str，日志文件的完整路径。

样例：

from mindspore.train.summary import SummaryRecord
if __name__ == '__main__':
    with SummaryRecord(log_dir="./summary_dir", file_prefix="xx_", file_suffix="_yy") as summary_record:
        log_dir = summary_record.log_dir
record(step, train_network=None, plugin_filter=None)[源代码]
记录summary。

参数：

step (int) - 表示当前的step。

train_network (Cell) - 表示用于保存计算图的训练网络。默认值：None，表示当原始网络的图为None时，不保存计算图。

plugin_filter (Callable[[str], bool], 可选) - 过滤器函数，用于过滤需要写入的标签项。默认值：None。

返回：

bool，表示记录是否成功。

异常：

TypeError： step 不为整型，或 train_network 的类型不为 mindspore.nn.Cell 。

样例：

from mindspore.train.summary import SummaryRecord
if __name__ == '__main__':
    with SummaryRecord(log_dir="./summary_dir", file_prefix="xx_", file_suffix="_yy") as summary_record:
        result = summary_record.record(step=2)
        print(result)


set_mode(mode)[源代码]
设置模型运行阶段。不同的阶段会影响记录数据的内容。

参数：

mode (str) - 待设置的网络阶段，可选值为”train”或”eval”。

train：代表训练阶段。

eval：代表评估阶段，此时 summary_record 不会记录summary算子的数据。

异常：

ValueError： mode 的值不在可选值内。

样例：

from mindspore.train.summary import SummaryRecord
if __name__ == '__main__':
    with SummaryRecord(log_dir="./summary_dir", file_prefix="xx_", file_suffix="_yy") as summary_record:
        summary_record.set_mode('eval')
mindspore.train.callback
classmindspore.train.callback.Callback[源代码]
用于构建Callback函数的基类。Callback函数是一个上下文管理器，在运行模型时被调用。 可以使用此机制进行一些自定义操作。

Callback函数可以在step或epoch开始前或结束后执行一些操作。 要创建自定义Callback，需要继承Callback基类并重载它相应的方法，有关自定义Callback的详细信息，请查看 Callback。

样例：

import numpy as np
from mindspore import Model, nn
from mindspore.train.callback import Callback
from mindspore import dataset as ds
class Print_info(Callback):
    def step_end(self, run_context):
        cb_params = run_context.original_args()
        print("step_num: ", cb_params.cur_step_num)

print_cb = Print_info()
data = {"x": np.float32(np.random.rand(64, 10)), "y": np.random.randint(0, 5, (64,))}
dataset = ds.NumpySlicesDataset(data=data).batch(32)
net = nn.Dense(10, 5)
loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')
optim = nn.Momentum(net.trainable_params(), 0.01, 0.9)
model = Model(net, loss_fn=loss, optimizer=optim)
model.train(1, dataset, callbacks=print_cb)

begin(run_context)[源代码]
在网络执行之前被调用一次。

参数：

run_context (RunContext) - 包含模型的一些基本信息。

end(run_context)[源代码]
网络执行后被调用一次。

参数：

run_context (RunContext) - 包含模型的一些基本信息。

epoch_begin(run_context)[源代码]
在每个epoch开始之前被调用。

参数：

run_context (RunContext) - 包含模型的一些基本信息。

epoch_end(run_context)[源代码]
在每个epoch结束后被调用。

参数：

run_context (RunContext) - 包含模型的一些基本信息。

step_begin(run_context)[源代码]
在每个step开始之前被调用。

参数：

run_context (RunContext) - 包含模型的一些基本信息。

step_end(run_context)[源代码]
在每个step完成后被调用。

参数：

run_context (RunContext) - 包含模型的一些基本信息。

classmindspore.train.callback.LossMonitor(per_print_times=1)[源代码]
监控训练的loss。

如果loss是NAN或INF，则终止训练。

Note

如果 per_print_times 为0，则不打印loss。

参数：

per_print_times (int) - 表示每隔多少个step打印一次loss。默认值：1。

异常：

ValueError - 当 per_print_times 不是整数或小于零。

样例：

from mindspore import Model, nn

net = LeNet5()
loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')
optim = nn.Momentum(net.trainable_params(), 0.01, 0.9)
model = Model(net, loss_fn=loss, optimizer=optim)
data_path = './MNIST_Data'
dataset = create_dataset(data_path)
loss_monitor = LossMonitor()
model.train(10, dataset, callbacks=loss_monitor)
step_end(run_context)[源代码]
step结束时打印训练loss。

参数：

run_context (RunContext) - 包含模型的相关信息。

classmindspore.train.callback.TimeMonitor(data_size=None)[源代码]
监控训练时间。

参数：

data_size (int) - 表示每隔多少个step打印一次信息。如果程序在训练期间获取到Model的 batch_num ，则将把 data_size 设为 batch_num ，否则将使用 data_size 。默认值：None。

异常：

ValueError - data_size 不是正整数。

样例：

from mindspore import Model, nn

net = LeNet5()
loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')
optim = nn.Momentum(net.trainable_params(), 0.01, 0.9)
model = Model(net, loss_fn=loss, optimizer=optim)
data_path = './MNIST_Data'
dataset = create_dataset(data_path)
time_monitor = TimeMonitor()
model.train(10, dataset, callbacks=time_monitor)
epoch_begin(run_context)[源代码]
在epoch开始时记录时间。

参数：

run_context (RunContext) - 包含模型的相关信息。

epoch_end(run_context)[源代码]
在epoch结束时打印epoch的耗时。

参数：

run_context (RunContext) - 包含模型的相关信息。

classmindspore.train.callback.ModelCheckpoint(prefix='CKP', directory=None, config=None)[源代码]
checkpoint的回调函数。

在训练过程中调用该方法可以保存网络参数。

Note

在分布式训练场景下，请为每个训练进程指定不同的目录来保存checkpoint文件。否则，可能会训练失败。 如果在 model 方法中使用此回调函数，默认将会把优化器中的参数保存到checkpoint文件中。

参数：

prefix (str) - checkpoint文件的前缀名称。默认值：’CKP’。

directory (str) - 保存checkpoint文件的文件夹路径。默认情况下，文件保存在当前目录下。默认值：None。

config (CheckpointConfig) - checkpoint策略配置。默认值：None。

异常：

ValueError - 如果prefix参数不是str类型或包含’/’字符。

ValueError - 如果directory参数不是str类型。

TypeError - config不是CheckpointConfig类型。

end(run_context)[源代码]
在训练结束后，会保存最后一个step的checkpoint。

参数：

run_context (RunContext) - 包含模型的一些基本信息。

propertylatest_ckpt_file_name
返回最新的checkpoint路径和文件名。

step_end(run_context)[源代码]
在step结束时保存checkpoint。

参数：

run_context (RunContext) - 包含模型的一些基本信息。

classmindspore.train.callback.SummaryCollector(summary_dir, collect_freq=10, collect_specified_data=None, keep_default_action=True, custom_lineage_data=None, collect_tensor_freq=None, max_file_size=None, export_options=None)[源代码]
SummaryCollector可以收集一些常用信息。

它可以帮助收集loss、学习率、计算图等。 SummaryCollector还可以允许通过 summary算子 将数据收集到summary文件中。

Note

使用SummaryCollector时，需要将代码放置到 if __name__ == “__main__” 中运行。

不允许在回调列表中存在多个SummaryCollector实例。

并非所有信息都可以在训练阶段或评估阶段收集。

SummaryCollector始终记录summary算子收集的数据。

SummaryCollector仅支持Linux系统。

编译MindSpore时，设置 -s on 关闭维测功能后，SummaryCollector不可用。

参数：

summary_dir (str) - 收集的数据将存储到此目录。如果目录不存在，将自动创建。

collect_freq (int) - 设置数据收集的频率，频率应大于零，单位为 step 。如果设置了频率，将在(current steps % freq)=0时收集数据，并且将总是收集第一个step。需要注意的是，如果使用数据下沉模式，单位将变成 epoch 。不建议过于频繁地收集数据，因为这可能会影响性能。默认值：10。

collect_specified_data (Union[None, dict]) - 对收集的数据进行自定义操作。您可以使用字典自定义需要收集的数据类型。例如，您可以设置{‘collect_metric’:False}不去收集metrics。支持控制的数据如下。默认值：None，收集所有数据。

collect_metric (bool) - 表示是否收集训练metrics，目前只收集loss。把第一个输出视为loss，并且算出其平均数。默认值：True。

collect_graph (bool) - 表示是否收集计算图。目前只收集训练计算图。默认值：True。

collect_train_lineage (bool) - 表示是否收集训练阶段的lineage数据，该字段将显示在MindInsight的 lineage页面 上。默认值：True。

collect_eval_lineage (bool) - 表示是否收集评估阶段的lineage数据，该字段将显示在MindInsight的lineage页面上。默认值：True。

collect_input_data (bool) - 表示是否为每次训练收集数据集。目前仅支持图像数据。如果数据集中有多列数据，则第一列应为图像数据。默认值：True。

collect_dataset_graph (bool) - 表示是否收集训练阶段的数据集图。默认值：True。

histogram_regular (Union[str, None]) - 收集参数分布页面的权重和偏置，并在MindInsight中展示。此字段允许正则表达式控制要收集的参数。不建议一次收集太多参数，因为这会影响性能。注：如果收集的参数太多并且内存不足，训练将会失败。默认值：None，表示只收集网络的前五个超参。

collect_landscape (Union[dict, None]) - 表示是否收集创建loss地形图所需要的参数。如果设置None，则不收集任何参数。默认收集所有参数并且将会保存在 {summary_dir}/ckpt_dir/train_metadata.json 文件中。

landscape_size (int) - 指定生成loss地形图的图像分辨率。例如：如果设置为128，则loss地形图的分辨率是128*128。注意：计算loss地形图的时间随着分辨率的增大而增加。默认值：40。可选值：3-256。

unit (str) - 指定训练过程中保存checkpoint时，下方参数 intervals 以何种形式收集模型权重。例如：将 intervals 设置为[[1, 2, 3, 4]]，如果 unit 设置为step，则收集模型权重的频率单位为step，将保存1-4个step的模型权重，而 unit 设置为epoch，则将保存1-4个epoch的模型权重。默认值：step。可选值：epoch/step。

create_landscape (dict) - 选择创建哪种类型的loss地形图，分为训练过程loss地形图（train）和训练结果loss地形图（result）。默认值：{“train”: True, “result”: True}。可选值：True/False。

num_samples (int) - 创建loss地形图所使用的数据集的大小。例如：在图像数据集中，您可以设置 num_samples 是128，这意味着将有128张图片被用来创建loss地形图。注意：num_samples 越大，计算loss地形图时间越长。默认值：128。

intervals (List[List[int]]) - 指定loss地形图的区间。例如：如果用户想要创建两张训练过程的loss地形图，分别为1-5epoch和6-10epoch，则用户可以设置[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]。注意：每个区间至少包含3个epoch。

keep_default_action (bool) - 此字段影响 collect_specified_data 字段的收集行为。True：表示设置指定数据后，其他数据按默认设置收集。False：表示设置指定数据后，只收集指定数据，不收集其他数据。默认值：True。

custom_lineage_data (Union[dict, None]) - 允许您自定义数据并将数据显示在MindInsight的lineage页面上。在自定义数据中，key支持str类型，value支持str、int和float类型。默认值：None，表示不存在自定义数据。

collect_tensor_freq (Optional[int]) - 语义与 collect_freq 的相同，但仅控制TensorSummary。由于TensorSummary数据太大，无法与其他summary数据进行比较，因此此参数用于降低收集量。默认情况下，收集TensorSummary数据的最大step数量为20，但不会超过收集其他summary数据的step数量。例如，给定 collect_freq=10 ，当总step数量为600时，TensorSummary将收集20个step，而收集其他summary数据时会收集61个step。但当总step数量为20时，TensorSummary和其他summary将收集3个step。另外请注意，在并行模式下，会平均分配总的step数量，这会影响TensorSummary收集的step的数量。默认值：None，表示要遵循上述规则。

max_file_size (Optional[int]) - 可写入磁盘的每个文件的最大大小（以字节为单位）。例如，如果不大于4GB，则设置 max_file_size=4*1024**3 。默认值：None，表示无限制。

export_options (Union[None, dict]) - 表示对导出的数据执行自定义操作。注：导出的文件的大小不受 max_file_size 的限制。您可以使用字典自定义导出的数据。例如，您可以设置{‘tensor_format’:’npy’}将tensor导出为 npy 文件。支持控制的数据如下所示。默认值：None，表示不导出数据。

tensor_format (Union[str, None]) - 自定义导出的tensor的格式。支持[“npy”, None]。默认值：None，表示不导出tensor。

npy - 将tensor导出为NPY文件。

异常：

ValueError： 编译MindSpore时，设置 -s on 关闭了维测功能。

样例：

import mindspore.nn as nn
from mindspore import context
from mindspore.train.callback import SummaryCollector
from mindspore import Model
from mindspore.nn import Accuracy

if __name__ == '__main__':
    # If the device_target is GPU, set the device_target to "GPU"
    context.set_context(mode=context.GRAPH_MODE, device_target="Ascend")
    mnist_dataset_dir = '/path/to/mnist_dataset_directory'
    # The detail of create_dataset method shown in model_zoo.official.cv.lenet.src.dataset.py
    ds_train = create_dataset(mnist_dataset_dir, 32)
    # The detail of LeNet5 shown in model_zoo.official.cv.lenet.src.lenet.py
    network = LeNet5(10)
    net_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction="mean")
    net_opt = nn.Momentum(network.trainable_params(), 0.01, 0.9)
    model = Model(network, net_loss, net_opt, metrics={"Accuracy": Accuracy()}, amp_level="O2")

    # Simple usage:
    summary_collector = SummaryCollector(summary_dir='./summary_dir')
    model.train(1, ds_train, callbacks=[summary_collector], dataset_sink_mode=False)

    # Do not collect metric and collect the first layer parameter, others are collected by default
    specified={'collect_metric': False, 'histogram_regular': '^conv1.*'}
    summary_collector = SummaryCollector(summary_dir='./summary_dir', collect_specified_data=specified)
    model.train(1, ds_train, callbacks=[summary_collector], dataset_sink_mode=False)
classmindspore.train.callback.CheckpointConfig(save_checkpoint_steps=1, save_checkpoint_seconds=0, keep_checkpoint_max=5, keep_checkpoint_per_n_minutes=0, integrated_save=True, async_save=False, saved_network=None, append_info=None, enc_key=None, enc_mode='AES-GCM', exception_save=False)[源代码]
保存checkpoint时的配置策略。

Note

在训练过程中，如果数据集是通过数据通道传输的，建议将 save_checkpoint_steps 设为循环下沉step数量的整数倍数，否则，保存checkpoint的时机可能会有偏差。建议同时只设置一种触发保存checkpoint策略和一种保留checkpoint文件总数策略。如果同时设置了 save_checkpoint_steps 和 save_checkpoint_seconds ，则 save_checkpoint_seconds 无效。如果同时设置了 keep_checkpoint_max 和 keep_checkpoint_per_n_minutes ，则 keep_checkpoint_per_n_minutes 无效。

参数：

save_checkpoint_steps (int) - 每隔多少个step保存一次checkpoint。默认值：1。

save_checkpoint_seconds (int) - 每隔多少秒保存一次checkpoint。不能同时与 save_checkpoint_steps 一起使用。默认值：0。

keep_checkpoint_max (int) - 最多保存多少个checkpoint文件。默认值：5。

keep_checkpoint_per_n_minutes (int) - 每隔多少分钟保存一个checkpoint文件。不能同时与 keep_checkpoint_max 一起使用。默认值：0。

integrated_save (bool) - 在自动并行场景下，是否合并保存拆分后的Tensor。合并保存功能仅支持在自动并行场景中使用，在手动并行场景中不支持。默认值：True。

async_save (bool) - 是否异步执行保存checkpoint文件。默认值：False。

saved_network (Cell) - 保存在checkpoint文件中的网络。如果 saved_network 没有被训练，则保存 saved_network 的初始值。默认值：None。

append_info (list) - 保存在checkpoint文件中的信息。支持”epoch_num”、”step_num”和dict类型。dict的key必须是str，dict的value必须是int、float或bool中的一个。默认值：None。

enc_key (Union[None, bytes]) - 用于加密的字节类型key。如果值为None，则不需要加密。默认值：None。

enc_mode (str) - 仅当 enc_key 不设为None时，该参数有效。指定了加密模式，目前支持AES-GCM和AES-CBC。默认值：AES-GCM。

exception_save (bool) - 当有异常发生时，是否保存当前checkpoint文件。默认值：False。

异常：

ValueError - 输入参数的类型不正确。

样例：

from mindspore import Model, nn
from mindspore.train.callback import ModelCheckpoint, CheckpointConfig
from mindspore.common.initializer import Normal

class LeNet5(nn.Cell):
    def __init__(self, num_class=10, num_channel=1):
        super(LeNet5, self).__init__()
        self.conv1 = nn.Conv2d(num_channel, 6, 5, pad_mode='valid')
        self.conv2 = nn.Conv2d(6, 16, 5, pad_mode='valid')
        self.fc1 = nn.Dense(16 * 5 * 5, 120, weight_init=Normal(0.02))
        self.fc2 = nn.Dense(120, 84, weight_init=Normal(0.02))
        self.fc3 = nn.Dense(84, num_class, weight_init=Normal(0.02))
        self.relu = nn.ReLU()
        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)
        self.flatten = nn.Flatten()

    def construct(self, x):
        x = self.max_pool2d(self.relu(self.conv1(x)))
        x = self.max_pool2d(self.relu(self.conv2(x)))
        x = self.flatten(x)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

net = LeNet5()
loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')
optim = nn.Momentum(net.trainable_params(), 0.01, 0.9)
model = Model(net, loss_fn=loss, optimizer=optim)
data_path = './MNIST_Data'
dataset = create_dataset(data_path)
config = CheckpointConfig(saved_network=net)
ckpoint_cb = ModelCheckpoint(prefix='LeNet5', directory='./checkpoint', config=config)
model.train(10, dataset, callbacks=ckpoint_cb)
propertyappend_dict
获取需要额外保存到checkpoint中的字典的值。

返回：

Dict: 字典中的值。

propertyasync_save
获取是否异步保存checkpoint。

返回：

Bool: 是否异步保存checkpoint。

propertyenc_key
获取加密的key值。

返回：

(None, bytes): 加密的key值。

propertyenc_mode
获取加密模式。

返回：

str: 加密模式。

get_checkpoint_policy()[源代码]
获取checkpoint的保存策略。

返回：

Dict: checkpoint的保存策略。

propertyintegrated_save
获取是否合并保存拆分后的Tensor。

返回：

Bool: 获取是否合并保存拆分后的Tensor。

propertykeep_checkpoint_max
获取最多保存checkpoint文件的数量。

返回：

Int: 最多保存checkpoint文件的数量。

propertykeep_checkpoint_per_n_minutes
获取每隔多少分钟保存一个checkpoint文件。

返回：

Int: 每隔多少分钟保存一个checkpoint文件。

propertysave_checkpoint_seconds
获取每隔多少秒保存一次checkpoint文件。

返回：

Int: 每隔多少秒保存一次checkpoint文件。

propertysave_checkpoint_steps
获取每隔多少个step保存一次checkpoint文件。

返回：

Int: 每隔多少个step保存一次checkpoint文件。

propertysaved_network
获取需要保存的网络。

返回：

Cell: 需要保存的网络。

classmindspore.train.callback.RunContext(original_args)[源代码]
提供模型的相关信息。

在Model方法里提供模型的相关信息。 回调函数可以调用 request_stop() 方法来停止迭代。

参数：

original_args (dict) - 模型的相关信息。

get_stop_requested()[源代码]
获取是否停止训练的标志。

返回：

bool，如果为True，则 Model.train() 停止迭代。

original_args()[源代码]
获取模型相关信息的对象。

返回：

dict，含有模型的相关信息的对象。

request_stop()[源代码]
在训练期间设置停止请求。

可以使用此函数请求停止训练。 Model.train() 会检查是否调用此函数。

classmindspore.train.callback.LearningRateScheduler(learning_rate_function)[源代码]
用于在训练期间更改学习率。

参数：

learning_rate_function (Function) - 在训练期间更改学习率的函数。

样例：

import numpy as np
from mindspore import Model
from mindspore.train.callback import LearningRateScheduler
import mindspore.nn as nn
from mindspore import dataset as ds

def learning_rate_function(lr, cur_step_num):
    if cur_step_num%1000 == 0:
        lr = lr*0.1
    return lr

lr = 0.1
momentum = 0.9
net = nn.Dense(10, 5)
loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')
optim = nn.Momentum(net.trainable_params(), learning_rate=lr, momentum=momentum)
model = Model(net, loss_fn=loss, optimizer=optim)

data = {"x": np.float32(np.random.rand(64, 10)), "y": np.random.randint(0, 5, (64,))}
dataset = ds.NumpySlicesDataset(data=data).batch(32)
model.train(1, dataset, callbacks=[LearningRateScheduler(learning_rate_function)],
            dataset_sink_mode=False)
step_end(run_context)[源代码]
在step结束时更改学习率。

参数：

run_context (RunContext) - 包含模型的一些基本信息。

classmindspore.train.callback.SummaryLandscape(summary_dir)[源代码]
SummaryLandscape可以帮助您收集loss地形图的信息。通过计算loss，可以在PCA（Principal Component Analysis）方向或者随机方向创建地形图。

Note

使用SummaryLandscape时，需要将代码放置到 if __name__ == “__main__” 中运行。

SummaryLandscape仅支持Linux系统。

参数：

summary_dir (str) - 该路径将被用来保存创建地形图所使用的数据。

样例：

import mindspore.nn as nn
from mindspore import context
from mindspore.train.callback import SummaryCollector, SummaryLandscape
from mindspore import Model
from mindspore.nn import Loss, Accuracy

if __name__ == '__main__':
    # If the device_target is Ascend, set the device_target to "Ascend"
    context.set_context(mode=context.GRAPH_MODE, device_target="GPU")
    mnist_dataset_dir = '/path/to/mnist_dataset_directory'
    # The detail of create_dataset method shown in model_zoo.official.cv.lenet.src.dataset.py
    ds_train = create_dataset(mnist_dataset_dir, 32)
    # The detail of LeNet5 shown in model_zoo.official.cv.lenet.src.lenet.py
    network = LeNet5(10)
    net_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction="mean")
    net_opt = nn.Momentum(network.trainable_params(), 0.01, 0.9)
    model = Model(network, net_loss, net_opt, metrics={"Accuracy": Accuracy()})
    # Simple usage for collect landscape information:
    interval_1 = [1, 2, 3, 4, 5]
    summary_collector = SummaryCollector(summary_dir='./summary/lenet_interval_1',
                                         collect_specified_data={'collect_landscape':{"landscape_size": 4,
                                                                                       "unit": "step",
                                                                         "create_landscape":{"train":True,
                                                                                            "result":False},
                                                                         "num_samples": 2048,
                                                                         "intervals": [interval_1]}
                                                                   })
    model.train(1, ds_train, callbacks=[summary_collector], dataset_sink_mode=False)

    # Simple usage for visualization landscape:
    def callback_fn():
        network = LeNet5(10)
        net_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction="mean")
        metrics = {"Loss": Loss()}
        model = Model(network, net_loss, metrics=metrics)
        mnist_dataset_dir = '/path/to/mnist_dataset_directory'
        ds_eval = create_dataset(mnist_dataset_dir, 32)
        return model, network, ds_eval, metrics

    summary_landscape = SummaryLandscape('./summary/lenet_interval_1')
    # parameters of collect_landscape can be modified or unchanged
    summary_landscape.gen_landscapes_with_multi_process(callback_fn,
                                                       collect_landscape={"landscape_size": 4,
                                                                        "create_landscape":{"train":False,
                                                                                           "result":False},
                                                                         "num_samples": 2048,
                                                                         "intervals": [interval_1]},
                                                        device_ids=[1])
clean_ckpt()[源代码]
清理checkpoint。

gen_landscapes_with_multi_process(callback_fn, collect_landscape=None, device_ids=None, output=None)[源代码]
使用多进程来生成地形图。

参数：

callback_fn (python function) - Python函数对象，用户需要写一个没有输入的函数，返回值要求如下。

mindspore.train.Model：用户的模型。

mindspore.nn.Cell：用户的网络。

mindspore.dataset：创建loss所需要的用户数据集。

mindspore.nn.Metrics：用户的评估指标。

collect_landscape (Union[dict, None]) - 创建loss地形图所用的参数含义与SummaryCollector同名字段一致。此处设置的目的是允许用户可以自由修改创建loss地形图参数。默认值：None。

landscape_size (int) - 指定生成loss地形图的图像分辨率。例如：如果设置为128，则loss地形图的分辨率是128*128。计算loss地形图的时间随着分辨率的增大而增加。默认值：40。可选值：3-256。

create_landscape (dict) - 选择创建哪种类型的loss地形图，分为训练过程loss地形图（train）和训练结果loss地形图（result）。默认值：{“train”: True, “result”: True}。可选值：True/False。

num_samples (int) - 创建loss地形图所使用的数据集的大小。例如：在图像数据集中，您可以设置 num_samples 是128，这意味着将有128张图片被用来创建loss地形图。注意：num_samples 越大，计算loss地形图时间越长。默认值：128。

intervals (List[List[int]]) - 指定创建loss地形图所需要的checkpoint区间。例如：如果用户想要创建两张训练过程的loss地形图，分别为1-5epoch和6-10epoch，则用户可以设置[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]。注意：每个区间至少包含3个epoch。

device_ids (List(int)) - 指定创建loss地形图所使用的目标设备的ID。例如：[0, 1]表示使用设备0和设备1来创建loss地形图。默认值：None。

output (str) - 指定保存loss地形图的路径。默认值：None。默认保存路径与summary文件相同。

classmindspore.train.callback.History[源代码]
将网络输出的相关信息记录到 History 对象中。

用户不自定义训练网络或评估网络情况下，记录的内容将为损失值；用户自定义了训练网络/评估网络的情况下，如果定义的网络返回 Tensor 或 numpy.ndarray，则记录此返回值均值，如果返回 tuple 或 list，则记录第一个元素。

Note

通常使用在 mindspore.Model.train 中。

样例：

import numpy as np
import mindspore.dataset as ds
from mindspore.train.callback import History
from mindspore import Model, nn
data = {"x": np.float32(np.random.rand(64, 10)), "y": np.random.randint(0, 5, (64,))}
train_dataset = ds.NumpySlicesDataset(data=data).batch(32)
net = nn.Dense(10, 5)
crit = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')
opt = nn.Momentum(net.trainable_params(), 0.01, 0.9)
history_cb = History()
model = Model(network=net, optimizer=opt, loss_fn=crit, metrics={"recall"})
model.train(2, train_dataset, callbacks=[history_cb])
print(history_cb.epoch)
print(history_cb.history)


begin(run_context)[源代码]
训练开始时初始化History对象的epoch属性。

参数：

run_context (RunContext) - 包含模型的一些基本信息。

epoch_end(run_context)[源代码]
epoch结束时记录网络输出的相关信息。

参数：

run_context (RunContext) - 包含模型的一些基本信息。

classmindspore.train.callback.LambdaCallback(epoch_begin=None, epoch_end=None, step_begin=None, step_end=None, begin=None, end=None)[源代码]
用于自定义简单的callback。

使用匿名函数构建callback，定义的匿名函数将在 mindspore.Model.{train | eval} 的对应阶段被调用。

请注意，callback的每个阶段都需要一个位置参数：run_context。

Note

这是一个会变更或删除的实验性接口。

参数：

epoch_begin (Function) - 每个epoch开始时被调用。

epoch_end (Function) - 每个epoch结束时被调用。

step_begin (Function) - 每个step开始时被调用。

step_end (Function) - 每个step结束时被调用。

begin (Function) - 模型训练、评估开始时被调用。

end (Function) - 模型训练、评估结束时被调用。

样例：

import numpy as np
import mindspore.dataset as ds
from mindspore.train.callback import LambdaCallback
from mindspore import Model, nn
data = {"x": np.float32(np.random.rand(64, 10)), "y": np.random.randint(0, 5, (64,))}
train_dataset = ds.NumpySlicesDataset(data=data).batch(32)
net = nn.Dense(10, 5)
crit = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')
opt = nn.Momentum(net.trainable_params(), 0.01, 0.9)
lambda_callback = LambdaCallback(epoch_end=
lambda run_context: print("loss: ", run_context.original_args().net_outputs))
model = Model(network=net, optimizer=opt, loss_fn=crit, metrics={"recall"})
model.train(2, train_dataset, callbacks=[lambda_callback])


mindspore.train.train_thor
转换为二阶相关的类和函数。

classmindspore.train.train_thor.ConvertNetUtils[源代码]
将网络转换为thor层网络，用于计算并存储二阶信息矩阵。

convert_to_thor_net(net)[源代码]
该接口用于将网络转换为thor层网络，用于计算并存储二阶信息矩阵。

Note

此接口由二阶优化器thor自动调用。

参数：

net (Cell) - 由二阶优化器thor训练的网络。

支持平台：

Ascend GPU

样例：

ConvertNetUtils().convert_to_thor_net(net)
classmindspore.train.train_thor.ConvertModelUtils[源代码]
该接口用于增加计算图，提升二阶算法THOR运行时的性能。

staticconvert_to_thor_model(model, network, loss_fn=None, optimizer=None, metrics=None, amp_level='O0', loss_scale_manager=None, keep_batchnorm_fp32=False)[源代码]
该接口用于增加计算图，提升二阶算法THOR运行时的性能。

参数：

model (Object) - 用于训练的高级API。 Model 将图层分组到具有训练特征的对象中。

network (Cell) - 训练网络。

loss_fn (Cell) - 目标函数。默认值：None。

optimizer (Cell) - 用于更新权重的优化器。默认值：None。

metrics (Union[dict, set]) - 在训练期间由模型评估的词典或一组度量。例如：{‘accuracy’, ‘recall’}。默认值：None。

amp_level (str) - 混合精度训练的级别。支持[“O0”, “O2”, “O3”, “auto”]。默认值：”O0”。

O0 - 不改变。

O2 - 将网络转换为float16，使用动态loss scale保持BN在float32中运行。

O3 - 将网络强制转换为float16，并使用附加属性 keep_batchnorm_fp32=False 。

auto - 在不同设备中，将级别设置为建议级别。GPU上建议使用O2，Ascend上建议使用O3。建议级别基于专家经验，不能总是一概而论。用户应指定特殊网络的级别。

loss_scale_manager (Union[None, LossScaleManager]) - 如果为None，则不会按比例缩放loss。否则，通过LossScaleManager和优化器缩放loss不能为None。这是一个关键参数。例如，使用 loss_scale_manager=None 设置值。

keep_batchnorm_fp32 (bool) - 保持BN在 float32 中运行。如果为True，则将覆盖之前的级别设置。默认值：False。

返回：

model (Object) - 用于训练的高级API。 Model 将图层分组到具有训练特征的对象中。

支持平台：

Ascend GPU

样例：

from mindspore import nn
from mindspore import Tensor
from mindspore.nn import thor
from mindspore import Model
from mindspore import FixedLossScaleManager
from mindspore.train.callback import LossMonitor
from mindspore.train.train_thor import ConvertModelUtils

net = Net()
dataset = create_dataset()
temp = Tensor([4e-4, 1e-4, 1e-5, 1e-5], mstype.float32)
opt = thor(net, learning_rate=temp, damping=temp, momentum=0.9, loss_scale=128, frequency=4)
loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')
loss_scale = FixedLossScaleManager(128, drop_overflow_update=False)
model = Model(net, loss_fn=loss, optimizer=opt, loss_scale_manager=loss_scale, metrics={'acc'},
              amp_level="O2", keep_batchnorm_fp32=False)
model = ConvertModelUtils.convert_to_thor_model(model=model, network=net, loss_fn=loss, optimizer=opt,
                                                loss_scale_manager=loss_scale, metrics={'acc'},
                                                amp_level="O2", keep_batchnorm_fp32=False)
loss_cb = LossMonitor()
model.train(1, dataset, callbacks=loss_cb, sink_size=4, dataset_sink_mode=True)